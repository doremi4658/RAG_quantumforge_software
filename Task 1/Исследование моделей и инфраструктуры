1. Сравнение LLM-моделей
Критерий	Локальные через Ollama (Mistral, LLaMA 3, Qwen)	Облачные (OpenAI / YandexGPT)
Качество	Mistral 7B / LLaMA 3 8B – близки к GPT-3.5, достаточно для RAG !	Высокое (GPT-4), но избыточно
Скорость	Зависит от GPU (∼30–50 токенов/сек на T4) !	1–3 сек (API)
Стоимость	Бесплатно (модели open-source)	! Плата за токены (∼$15/мес при 1000 запросах)
Развёртывание	Ollama – просто (одна команда), требуется GPU	! Просто (API-ключ), не требует GPU
Конфиденциальность	Полная (данные не покидают сервер)	! Данные уходят провайдеру
Вывод: Выбираем локальные модели через Ollama – бесплатно, безопасно, качество приемлемо. Потребуется GPU.

2. Сравнение моделей эмбеддингов
Критерий	Локальные (Sentence-Transformers)	! Облачные (OpenAI Embeddings)
Скорость индексации	CPU/GPU, ∼10–50 док/сек	! Лимиты API (∼3000 RPM)
Качество поиска	intfloat/multilingual-e5-small – отличное, многоязычное	 ! text-embedding-ada-002 – отличное
Стоимость	Бесплатно !	$0.13/1M токенов
Конфиденциальность	Полная !	Данные уходят в облако
Выбор: intfloat/multilingual-e5-small – бесплатно, безопасно, хорошо для русского и английского.

3. Сравнение векторных БД
Критерий	FAISS !	ChromaDB
Скорость поиска	Очень высокая	! Высокая (HNSW)
Сложность внедрения	Выше (метаданные отдельно)	! Низкая (простое API, метаданные, персистентность)
Удобство обновления	Сложно (требуется перестроение) !	Легко (добавление/удаление)
Стоимость	Бесплатно !	Бесплатно
Выбор: ChromaDB – простота работы, необходимая для автоматического обновления (задание 6).

4. Рекомендуемая конфигурация сервера
Для локальной LLM (например, Mistral 7B в 4-bit) требуется GPU:

GPU: 8–12 ГБ VRAM (NVIDIA T4, RTX 3060 12GB).

CPU: 4 ядра (для бота, эмбеддингов, сервисов).

RAM: 16 ГБ.

Диск: 50 ГБ SSD.


5. Итоговые рекомендации
Вариант А (локальный, оптимальный) – рекомендуемый
LLM: Mistral 7B (или LLaMA 3 8B) через Ollama (квантованная 4-bit).

Эмбеддинги: intfloat/multilingual-e5-small (локально).

Векторная БД: ChromaDB.

Сервер: с GPU T4 / RTX 3060.

Плюсы: полностью бесплатно (кроме аренды GPU), полная конфиденциальность, простота развёртывания (Ollama), отличное качество для RAG.

Минусы: нужен GPU, немного медленнее облачных API.

Вариант Б (ультрабюджетный, CPU-only)
LLM: TinyLlama 1.1B через Ollama на CPU.

Эмбеддинги: all-MiniLM-L6-v2 (лёгкая).

Векторная БД: ChromaDB.

Сервер: CPU 4 ядра, 8 ГБ RAM (без GPU).

Плюсы: не требует GPU, можно запустить даже на слабой машине.

Минусы: заметно хуже качество ответов, подходит только для демонстрации концепции.

Вариант В (гибридный)
LLM: OpenAI GPT-3.5 через API.

Эмбеддинги: локальные e5-small.

Векторная БД: ChromaDB.

Сервер: CPU-only.

Плюсы: высокое качество без GPU, конфиденциальность документов.

Минусы: плата за API (∼$15/мес).

В обучение выбираю полностью локальный вариант А.
